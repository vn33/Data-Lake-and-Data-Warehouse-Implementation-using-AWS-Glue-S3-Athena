{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise: Data Lake with AWS Glue\n",
    "\n",
    "In this lab, you will work with a simple data lake that uses Amazon S3 as its primary storage. The data lake bucket contains raw JSON files that you will transform using AWS Glue ETL, and then store the processed data in the same bucket. You will finally use AWS Glue crawler to populate the Glue data catalog with metadata about your processed data, and then use Amazon Athena to query your data using SQL statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [ 1 - Introduction](#1)\n",
    "- [ 2 - Raw Data Exploration](#2)\n",
    "  - [ Exercise 1](#ex01)\n",
    "  - [ Exercise 2](#ex02)\n",
    "- [ 3 - Raw Data Processing](#3)\n",
    "  - [ Exercise 3](#ex03)\n",
    "  - [ Exercise 4](#ex04)\n",
    "- [ 4 - Transformation with AWS Glue ETL](#4)\n",
    "  - [ 4.1 - Preparing the Scripts](#4.1)\n",
    "    - [ Exercise 5](#ex05)\n",
    "    - [ Exercise 6](#ex06)\n",
    "  - [ 4.2 - Creating and Running the Glue Jobs](#4.2)\n",
    "- [ 5 - Data Catalog with Glue Crawler](#5)\n",
    "  - [ Exercise 7](#ex07)\n",
    "- [ 6 - Data Querying with Athena](#6)\n",
    "- [ 7 - Optional Experiments: Partitioning and Compression Features of Parquet Format](#7)\n",
    "  - [ 7.1 - Experiments with the Glue Jobs](#7.1)\n",
    "  - [ 7.2 - Experiment Results](#7.2)\n",
    "- [ 8 - Clean up](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import some relevant libraries for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import gzip\n",
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import smart_open\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you work as a Data Engineer at a retailer specializing in scale models of classic cars and other transportation. The data analysts are interested in conducting trend analysis for the top products reviewed in Amazon, to inform new product development. Recently, your team acquired Amazon toy review data and product info, and stored them in a data lake bucket. You are asked to clean the data and ensure its accessibility, so that the data analysts can retrieve the data with SQL-based queries. For the initial testing phase, the team has opted to use AWS Glue ETL for the initial data cleaning, and Amazon Athena to query the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will use [Terraform](https://www.terraform.io/) to define the Glue ETL jobs. You will run these jobs to process the raw datasets and store the results as Parquet files in the same bucket. Using `boto3`, you will create a crawler that you will run over the processed data. The crawler will populate two catalog tables, each linked to a Parquet file. Finally you will query the data using Amazon Athena.\n",
    "\n",
    "<img src=\"images/data_lake.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Raw Data Exploration\n",
    "\n",
    "The dataset consists of two JSON compressed files, one for reviews and one for metadata of the products. Here is an example of one review JSON object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"reviewerID\": \"A2SUAM1J3GNN3B\",\n",
    "  \"asin\": \"0000013714\",\n",
    "  \"reviewerName\": \"J. McDonald\",\n",
    "  \"helpful\": [2, 3],\n",
    "  \"reviewText\": \"I bought this for my husband who plays the piano.  He is having a wonderful time playing these old hymns.  The music  is at times hard to read because we think the book was published for singing from more than playing from.  Great purchase though!\",\n",
    "  \"overall\": 5.0,\n",
    "  \"summary\": \"Heavenly Highway Hymns\",\n",
    "  \"unixReviewTime\": 1252800000,\n",
    "  \"reviewTime\": \"09 13, 2009\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the description of the fields:\n",
    "\n",
    "- `reviewerID` - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "- `asin` - ID of the product, e.g. 0000013714\n",
    "- `reviewerName` - name of the reviewer\n",
    "- `helpful` - helpfulness rating of the review, e.g. 2/3\n",
    "- `reviewText` - text of the review\n",
    "- `overall` - rating of the product\n",
    "- `summary` - summary of the review\n",
    "- `unixReviewTime` - time of the review (unix time)\n",
    "- `reviewTime` - time of the review (raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An here's an example of one product metadata JSON object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"asin\": \"0641843224\",\n",
    "  \"description\": \"Set your phasers to stun and prepare for a warp speed ride through the most memorable vocabulary from the sci-fi/fantasy genre.\",\n",
    "  \"title\": \"McNeill Designs YBS Sci-fi/Fantasy Add-on Deck\", \n",
    "  \"price\": 5.19,  \n",
    "  \"imUrl\": \"http://ecx.images-amazon.com/images/I/418t9AN9hiL._SY300_.jpg\", \n",
    "  \"related\": \n",
    "  {\n",
    "    \"also_bought\": [\"B000EVLZ9U\", \"0641843208\", \"0641843216\", \"0641843267\", \"1450751210\", \"0641843232\", \"B00ALQFYGI\", \"B004G7B3NQ\", \"B002PDM288\", \"B009ZNJZV8\", \"B009YG928W\", \"B0063NC3N0\"], \n",
    "    \"also_viewed\": [\"B000EVLZ9U\", \"1450751210\", \"0641843208\", \"0641843267\", \"0641843232\", \"0641843216\", \"B003EIK136\", \"B004G7B3NQ\", \"B003N2Q5JC\"], \n",
    "    \"bought_together\": [\"B000EVLZ9U\"]\n",
    "  },\n",
    "  \"salesRank\": {\"Toys & Games\": 154868}, \n",
    "  \"brand\": \"McNeill Designs\", \n",
    "  \"categories\": [[\"Toys & Games\", \"Games\", \"Card Games\"]]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following fields:\n",
    "\n",
    "- `asin` - ID of the product, e.g. 0000031852\n",
    "- `description` - Description of the product\n",
    "- `title` - name of the product\n",
    "- `price` - price in US dollars (at time of crawl)\n",
    "- `imUrl` - url of the product image\n",
    "- `related` - related products (`also_bought`, `also_viewed`, `bought_together`)\n",
    "- `salesRank` - sales rank information\n",
    "- `brand` - brand name\n",
    "- `categories` - list of categories the product belongs to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the name of the data lake bucket, go to the AWS console and click on the upper right part, where your username appears. Copy the **Account ID**. In the code below, set the variable `BUCKET_NAME` by replacing `<AWS-ACCOUNT-ID>` placeholder with the Account ID that you copied. The Account ID should contain only numbers without hyphens between them (e.g. 123412341234, not 1234-1234-1234 and the bucket name should have the same format as `de-c3w2lab1-123412341234-us-east-1-data-lake`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'de-c3w2lab1-533267389325-us-east-1-data-lake'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data lake bucket contains the raw JSON files. The bucket also contains samples of each dataset, which are of smaller size of the original data, that you will next interact with to explore the content of the data. The next cell consists of a function that you will use to load the data samples into a Pandas DataFrame so you can explore them. Run the following cell to define the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_sample(bucket_name: str, s3_file_key: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads review sample dataset\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): Bucket name\n",
    "        s3_file_key (str): Dataset s3 key location\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Read dataframe\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    source_uri = f's3://{bucket_name}/{s3_file_key}'\n",
    "    json_list = []\n",
    "    for json_line in smart_open.open(source_uri, transport_params={'client': s3_client}):\n",
    "        json_list.append(json.loads(json_line))\n",
    "    df = pd.DataFrame(json_list)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex01'></a>\n",
    "### Exercise 1\n",
    "\n",
    "Complete the code to call the `read_data_sample()` function passing the `BUCKET_NAME` and the file key parameter as `'staging/reviews_Toys_and_Games_sample.json.gz'`. Then, take a look at the first 5 rows of this sample reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMEVO2LY6VEJA</td>\n",
       "      <td>0000191639</td>\n",
       "      <td>Nicole Soeder</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Great product, thank you! Our son loved the pu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Puzzles</td>\n",
       "      <td>1388016000</td>\n",
       "      <td>12 26, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3C9CSW3TJITGT</td>\n",
       "      <td>0005069491</td>\n",
       "      <td>Renee</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I love these felt nursery rhyme characters and...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Charming characters but busy work required</td>\n",
       "      <td>1377561600</td>\n",
       "      <td>08 27, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A31POTIYCKSZ9G</td>\n",
       "      <td>0076561046</td>\n",
       "      <td>So CA Teacher</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I see no directions for its use. Therefore I h...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No directions for use...</td>\n",
       "      <td>1404864000</td>\n",
       "      <td>07 9, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2GGHHME9B6W4O</td>\n",
       "      <td>0131358936</td>\n",
       "      <td>Dalilah G.</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This is a great tool for any teacher using the...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great CD-ROM</td>\n",
       "      <td>1382400000</td>\n",
       "      <td>10 22, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1FSLDH43ORWZP</td>\n",
       "      <td>0133642984</td>\n",
       "      <td>Dayna English</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Although not as streamlined as the Algebra I m...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Algebra II -- presentation materials</td>\n",
       "      <td>1374278400</td>\n",
       "      <td>07 20, 2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin   reviewerName helpful  \\\n",
       "0   AMEVO2LY6VEJA  0000191639  Nicole Soeder  [0, 0]   \n",
       "1  A3C9CSW3TJITGT  0005069491          Renee  [0, 0]   \n",
       "2  A31POTIYCKSZ9G  0076561046  So CA Teacher  [0, 0]   \n",
       "3  A2GGHHME9B6W4O  0131358936     Dalilah G.  [0, 0]   \n",
       "4  A1FSLDH43ORWZP  0133642984  Dayna English  [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  Great product, thank you! Our son loved the pu...      5.0   \n",
       "1  I love these felt nursery rhyme characters and...      4.0   \n",
       "2  I see no directions for its use. Therefore I h...      3.0   \n",
       "3  This is a great tool for any teacher using the...      5.0   \n",
       "4  Although not as streamlined as the Algebra I m...      5.0   \n",
       "\n",
       "                                      summary  unixReviewTime   reviewTime  \n",
       "0                                     Puzzles      1388016000  12 26, 2013  \n",
       "1  Charming characters but busy work required      1377561600  08 27, 2013  \n",
       "2                    No directions for use...      1404864000   07 9, 2014  \n",
       "3                                Great CD-ROM      1382400000  10 22, 2013  \n",
       "4        Algebra II -- presentation materials      1374278400  07 20, 2013  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START CODE HERE ### (1 line of code)\n",
    "review_sample_df = read_data_sample(bucket_name=BUCKET_NAME, s3_file_key='staging/reviews_Toys_and_Games_sample.json.gz')\n",
    "### END CODE HERE ###\n",
    "\n",
    "review_sample_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data type of each of the columns of the dataset, and pay closer attention to the `unixReviewTime` and `helpful` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewerID         object\n",
       "asin               object\n",
       "reviewerName       object\n",
       "helpful            object\n",
       "reviewText         object\n",
       "overall           float64\n",
       "summary            object\n",
       "unixReviewTime      int64\n",
       "reviewTime         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_sample_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the `unixReviewTime` value is currently an integer that represents a Unix timestamp defined in terms of the number of seconds since January 1st, 1970 at UTC. Each `helpful` column consists of two numbers: the number of users who found the review helpful and the total number of users who rated the helpfulness of this review. Later in Exercise 3, you will transform this raw data to make it more useful for further analysis.\n",
    "\n",
    "Let's check out the metadata dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex02'></a>\n",
    "### Exercise 2\n",
    "\n",
    "Execute the `read_data_sample()` function with the same bucket name, but now set the file key parameter as `'staging/meta_Toys_and_Games_sample.json.gz'`. Then take a look at the first 5 rows of this sample metadata dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>salesRank</th>\n",
       "      <th>imUrl</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000191639</td>\n",
       "      <td>Three Dr. Suess' Puzzles: Green Eggs and Ham, ...</td>\n",
       "      <td>Dr. Suess 19163 Dr. Seuss Puzzle 3 Pack Bundle</td>\n",
       "      <td>37.12</td>\n",
       "      <td>{'Toys &amp; Games': 612379}</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/414PLROX...</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>[[Toys &amp; Games, Puzzles, Jigsaw Puzzles]]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0005069491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nursery Rhymes Felt Book</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Toys &amp; Games': 576683}</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51z4JDBC...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Toys &amp; Games]]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0076561046</td>\n",
       "      <td>Learn Fractions Decimals Percents using flash ...</td>\n",
       "      <td>Fraction Decimal Percent Card Deck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Toys &amp; Games': 564211}</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51ObabPu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Toys &amp; Games, Learning &amp; Education, Flash Ca...</td>\n",
       "      <td>{'also_viewed': ['0075728680']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0131358936</td>\n",
       "      <td>New, Sealed. Fast Shipping with tracking, buy ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.22</td>\n",
       "      <td>{'Software': 8080}</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51%2B7Ej...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[Toys &amp; Games, Learning &amp; Education, Mathemat...</td>\n",
       "      <td>{'also_bought': ['0321845536', '0078787572'], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0133642984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algebra 2 California Teacher Center</td>\n",
       "      <td>731.93</td>\n",
       "      <td>{'Toys &amp; Games': 1150291}</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51VK%2BL...</td>\n",
       "      <td>Prentice Hall</td>\n",
       "      <td>[[Toys &amp; Games, Learning &amp; Education, Mathemat...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                        description  \\\n",
       "0  0000191639  Three Dr. Suess' Puzzles: Green Eggs and Ham, ...   \n",
       "1  0005069491                                                NaN   \n",
       "2  0076561046  Learn Fractions Decimals Percents using flash ...   \n",
       "3  0131358936  New, Sealed. Fast Shipping with tracking, buy ...   \n",
       "4  0133642984                                                NaN   \n",
       "\n",
       "                                            title   price  \\\n",
       "0  Dr. Suess 19163 Dr. Seuss Puzzle 3 Pack Bundle   37.12   \n",
       "1                        Nursery Rhymes Felt Book     NaN   \n",
       "2              Fraction Decimal Percent Card Deck     NaN   \n",
       "3                                             NaN   36.22   \n",
       "4             Algebra 2 California Teacher Center  731.93   \n",
       "\n",
       "                   salesRank  \\\n",
       "0   {'Toys & Games': 612379}   \n",
       "1   {'Toys & Games': 576683}   \n",
       "2   {'Toys & Games': 564211}   \n",
       "3         {'Software': 8080}   \n",
       "4  {'Toys & Games': 1150291}   \n",
       "\n",
       "                                               imUrl          brand  \\\n",
       "0  http://ecx.images-amazon.com/images/I/414PLROX...      Dr. Seuss   \n",
       "1  http://ecx.images-amazon.com/images/I/51z4JDBC...            NaN   \n",
       "2  http://ecx.images-amazon.com/images/I/51ObabPu...            NaN   \n",
       "3  http://ecx.images-amazon.com/images/I/51%2B7Ej...            NaN   \n",
       "4  http://ecx.images-amazon.com/images/I/51VK%2BL...  Prentice Hall   \n",
       "\n",
       "                                          categories  \\\n",
       "0          [[Toys & Games, Puzzles, Jigsaw Puzzles]]   \n",
       "1                                   [[Toys & Games]]   \n",
       "2  [[Toys & Games, Learning & Education, Flash Ca...   \n",
       "3  [[Toys & Games, Learning & Education, Mathemat...   \n",
       "4  [[Toys & Games, Learning & Education, Mathemat...   \n",
       "\n",
       "                                             related  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                    {'also_viewed': ['0075728680']}  \n",
       "3  {'also_bought': ['0321845536', '0078787572'], ...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START CODE HERE ### (1 line of code)\n",
    "metadata_sample_df = read_data_sample(bucket_name=BUCKET_NAME, s3_file_key='staging/meta_Toys_and_Games_sample.json.gz')\n",
    "### END CODE HERE ###\n",
    "\n",
    "metadata_sample_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, take a look at the column's data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin            object\n",
       "description     object\n",
       "title           object\n",
       "price          float64\n",
       "salesRank       object\n",
       "imUrl           object\n",
       "brand           object\n",
       "categories      object\n",
       "related         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_sample_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this data more useful for analysis, later in Exercise 4 you will perform some transformations to the data. For example, you will extract the sales rank and category from the `salesRank` column and save them into two separate columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Raw Data Processing\n",
    "\n",
    "In this section, you will complete two processing functions that you will later incorporate into the Glue job scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex03'></a>\n",
    "### Exercise 3\n",
    "\n",
    "In this exercise, you will perform some transformations on the reviews dataset to make the data from the `unixReviewTime` and `helpful` columns more useful for analysis. \n",
    "\n",
    "Complete the `process_review()` function with the following transformations:\n",
    "\n",
    "1. Convert the `unixReviewTime` column to date with the `pd.to_datetime()` function. Remember that this timestamp is defined in seconds (use `s` for the `unit` parameter). Save the result in the column `reviewTime`.\n",
    "2. Extract the year and month from the `reviewTime`, and save those values in new columns named `year` and `month`. You can apply `.dt.year` and `.dt.month` methods to `raw_df['reviewTime']` to do that. You will later use these columns to partition the processed data in the `S3` bucket.\n",
    "3. Create a new DataFrame named `df_helpful` based on converting the `helpful` column from the `raw_df` into a list with the `to_list()` method. Set the column names as `helpful` and `totalHelpful`. \n",
    "4. With the `pd.concat()` function, concatenate the `raw_df` dataframe with `df_helpful`. From the `raw_df` drop the `helpful` column using the `raw_df.drop()` function, set `axis=1`.\n",
    "\n",
    "Then, perform the transformations and check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>helpful</th>\n",
       "      <th>totalHelpful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMEVO2LY6VEJA</td>\n",
       "      <td>0000191639</td>\n",
       "      <td>Nicole Soeder</td>\n",
       "      <td>Great product, thank you! Our son loved the pu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Puzzles</td>\n",
       "      <td>1388016000</td>\n",
       "      <td>2013-12-26</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3C9CSW3TJITGT</td>\n",
       "      <td>0005069491</td>\n",
       "      <td>Renee</td>\n",
       "      <td>I love these felt nursery rhyme characters and...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Charming characters but busy work required</td>\n",
       "      <td>1377561600</td>\n",
       "      <td>2013-08-27</td>\n",
       "      <td>2013</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A31POTIYCKSZ9G</td>\n",
       "      <td>0076561046</td>\n",
       "      <td>So CA Teacher</td>\n",
       "      <td>I see no directions for its use. Therefore I h...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No directions for use...</td>\n",
       "      <td>1404864000</td>\n",
       "      <td>2014-07-09</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2GGHHME9B6W4O</td>\n",
       "      <td>0131358936</td>\n",
       "      <td>Dalilah G.</td>\n",
       "      <td>This is a great tool for any teacher using the...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great CD-ROM</td>\n",
       "      <td>1382400000</td>\n",
       "      <td>2013-10-22</td>\n",
       "      <td>2013</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1FSLDH43ORWZP</td>\n",
       "      <td>0133642984</td>\n",
       "      <td>Dayna English</td>\n",
       "      <td>Although not as streamlined as the Algebra I m...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Algebra II -- presentation materials</td>\n",
       "      <td>1374278400</td>\n",
       "      <td>2013-07-20</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin   reviewerName  \\\n",
       "0   AMEVO2LY6VEJA  0000191639  Nicole Soeder   \n",
       "1  A3C9CSW3TJITGT  0005069491          Renee   \n",
       "2  A31POTIYCKSZ9G  0076561046  So CA Teacher   \n",
       "3  A2GGHHME9B6W4O  0131358936     Dalilah G.   \n",
       "4  A1FSLDH43ORWZP  0133642984  Dayna English   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  Great product, thank you! Our son loved the pu...      5.0   \n",
       "1  I love these felt nursery rhyme characters and...      4.0   \n",
       "2  I see no directions for its use. Therefore I h...      3.0   \n",
       "3  This is a great tool for any teacher using the...      5.0   \n",
       "4  Although not as streamlined as the Algebra I m...      5.0   \n",
       "\n",
       "                                      summary  unixReviewTime reviewTime  \\\n",
       "0                                     Puzzles      1388016000 2013-12-26   \n",
       "1  Charming characters but busy work required      1377561600 2013-08-27   \n",
       "2                    No directions for use...      1404864000 2014-07-09   \n",
       "3                                Great CD-ROM      1382400000 2013-10-22   \n",
       "4        Algebra II -- presentation materials      1374278400 2013-07-20   \n",
       "\n",
       "   year  month  helpful  totalHelpful  \n",
       "0  2013     12        0             0  \n",
       "1  2013      8        0             0  \n",
       "2  2014      7        0             0  \n",
       "3  2013     10        0             0  \n",
       "4  2013      7        0             0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_review(raw_df: pd.DataFrame) -> pd.DataFrame:    \n",
    "    \"\"\"Transformations steps for Reviews dataset\n",
    "\n",
    "    Args:\n",
    "        raw_df (DataFrame): Raw data loaded in dataframe\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Returned transformed dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (5 lines of code)\n",
    "    raw_df['reviewTime'] = pd.to_datetime(raw_df['unixReviewTime'], unit='s')\n",
    "    raw_df['year'] = raw_df['reviewTime'].dt.year\n",
    "    raw_df['month'] = raw_df['reviewTime'].dt.month\n",
    "    \n",
    "    df_helpful = pd.DataFrame(raw_df['helpful'].to_list(), columns=['helpful', 'totalHelpful'])\n",
    "    target_df = pd.concat([raw_df.drop(columns=['helpful']), df_helpful], axis=1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return target_df\n",
    "\n",
    "transformed_review_sample_df = process_review(raw_df=review_sample_df)\n",
    "transformed_review_sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex04'></a>\n",
    "### Exercise 4\n",
    "\n",
    "In this exercise, you will perform some transformations on the reviews metadata dataset to make the data from the `salesRank` column more useful for analysis. You will also remove some of the null values and replace others with an empty string.\n",
    "\n",
    "Complete the function provided in the next cell with the following steps: \n",
    "\n",
    "1. Remove any records that have null values for the `salesRank` column. You can use the `dropna()` method with the parameter `how=\"any\"`. Save the resulting dataframe in the `tmp_df` variable.\n",
    "2. Create a dataframe named `df_rank`. This dataframe should contain two columns named `'sales_category'` and `'sales_rank'` that come from extracting the key and value respectively from the `'salesRank'` column.\n",
    "3. Concatenate the `tmp_df` and `df_rank` dataframes and save the result in a new dataframe named `target_df`. You can use function `pd.concat()` to do that.\n",
    "4. Use the parameter `cols` to select the desired columns in the dataframe `target_df`.\n",
    "5. From `target_df`, remove any records that have null values for the `asin`, `price` and `sales_rank` columns. Again, use the `dropna()` method with the parameter `how=\"any\"`.\n",
    "6. Fill the null value of the rest of the dataframe with an empty string `\"\"`. You can use `fillna()` function to do that.\n",
    "\n",
    "Then, perform the transformations and check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>brand</th>\n",
       "      <th>sales_category</th>\n",
       "      <th>sales_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000191639</td>\n",
       "      <td>Three Dr. Suess' Puzzles: Green Eggs and Ham, ...</td>\n",
       "      <td>Dr. Suess 19163 Dr. Seuss Puzzle 3 Pack Bundle</td>\n",
       "      <td>37.12</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>612379.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0131358936</td>\n",
       "      <td>New, Sealed. Fast Shipping with tracking, buy ...</td>\n",
       "      <td></td>\n",
       "      <td>36.22</td>\n",
       "      <td></td>\n",
       "      <td>Software</td>\n",
       "      <td>8080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0133642984</td>\n",
       "      <td></td>\n",
       "      <td>Algebra 2 California Teacher Center</td>\n",
       "      <td>731.93</td>\n",
       "      <td>Prentice Hall</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>1150291.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0375829695</td>\n",
       "      <td>A collection of six 48-piece (that is,slightly...</td>\n",
       "      <td>Dr. Seuss Jigsaw Puzzle Book: With Six 48-Piec...</td>\n",
       "      <td>24.82</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>Home &amp;amp; Kitchen</td>\n",
       "      <td>590975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0439400066</td>\n",
       "      <td>Send your mind into overdrive with this mind-b...</td>\n",
       "      <td>3D Puzzle Buster</td>\n",
       "      <td>99.15</td>\n",
       "      <td></td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>1616332.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          asin                                        description  \\\n",
       "0   0000191639  Three Dr. Suess' Puzzles: Green Eggs and Ham, ...   \n",
       "3   0131358936  New, Sealed. Fast Shipping with tracking, buy ...   \n",
       "4   0133642984                                                      \n",
       "6   0375829695  A collection of six 48-piece (that is,slightly...   \n",
       "10  0439400066  Send your mind into overdrive with this mind-b...   \n",
       "\n",
       "                                                title   price          brand  \\\n",
       "0      Dr. Suess 19163 Dr. Seuss Puzzle 3 Pack Bundle   37.12      Dr. Seuss   \n",
       "3                                                       36.22                  \n",
       "4                 Algebra 2 California Teacher Center  731.93  Prentice Hall   \n",
       "6   Dr. Seuss Jigsaw Puzzle Book: With Six 48-Piec...   24.82      Dr. Seuss   \n",
       "10                                   3D Puzzle Buster   99.15                  \n",
       "\n",
       "        sales_category  sales_rank  \n",
       "0         Toys & Games    612379.0  \n",
       "3             Software      8080.0  \n",
       "4         Toys & Games   1150291.0  \n",
       "6   Home &amp; Kitchen    590975.0  \n",
       "10        Toys & Games   1616332.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_metadata(raw_df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Function in charge of the transformation of the raw data of the\n",
    "    Reviews Metadata.\n",
    "\n",
    "    Args:\n",
    "        raw_df (DataFrame): Raw data loaded in dataframe\n",
    "        cols (list): List of columns to select\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Returned transformed dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (6 lines of code)\n",
    "    tmp_df = raw_df.dropna(subset=[\"salesRank\"], how=\"any\")\n",
    "    \n",
    "    df_rank = pd.DataFrame([{\"sales_category\": key, \"sales_rank\": value} for d in tmp_df[\"salesRank\"].tolist() for key, value in d.items()])\n",
    "    \n",
    "    target_df = pd.concat([tmp_df, df_rank], axis=1)\n",
    "    target_df = target_df[cols]\n",
    "    target_df = target_df.dropna(subset=[\"asin\", \"price\", \"sales_rank\"], how=\"any\")\n",
    "    target_df = target_df.fillna(\"\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return target_df\n",
    "\n",
    "processed_metadata_df = process_metadata(raw_df=metadata_sample_df, \n",
    "                                         cols=['asin', 'description', 'title', 'price', 'brand','sales_category','sales_rank']\n",
    "                                         )\n",
    "processed_metadata_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have explored and performed some transformations on the reviews and metadata sample datasets, and you are now ready to perform those transformations over the complete dataset. This process will be done with AWS Glue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Transformation with AWS Glue ETL\n",
    "\n",
    "AWS Glue ETL (Extract, Transform, Load) is a serverless service that simplifies data integration and processing. You briefly used Glue in the labs of course 1. In this lab, you will interact more closely with this service, and in course 4, you will learn more details about its underlying distributed framework (Apache Spark). \n",
    "\n",
    "AWS Glue requires a Spark script to perform a job, this script can be coded in Python or Scala. Here you are provided with Python scripts. In these scripts, you will extract the raw data from the provided bucket, transform the data and then save it in the **parquet** format. Parquet is a columnar storage file format commonly used in big data processing frameworks like Apache Hadoop and Apache Spark. Parquet format has several features such as the support of compression algorithms and support of schema evolution. If you want to know more about this format, you can read [this article](https://airbyte.com/data-engineering-resources/parquet-data-format)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.1'></a>\n",
    "### 4.1 - Preparing the Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex05'></a>\n",
    "### Exercise 5\n",
    "\n",
    "You will now complete the script to transform the Amazon Reviews.\n",
    "\n",
    "1. Open the file `terraform/assets/de-c3w2-reviews-transform-job.py`. The script reads the data from the JSON file, performs basic transformations, and saves the result in a parquet file.\n",
    "\n",
    "2. Before completing the incomplete `transform()` function, read through the entire code and comments to get an overview of how the three steps - Extract, Transform, and Load - are implemented.\n",
    "\n",
    "3. Complete the `transform()` function by copying part of the code you completed in [Exercise 3](#ex03).\n",
    "\n",
    "4. Save changes to the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex06'></a>\n",
    "### Exercise 6\n",
    "\n",
    "You will now complete the script to transform the Amazon Product Metadata.\n",
    "\n",
    "1. Open the file `terraform/assets/de-c3w2-metadata-transform-job.py`.\n",
    "\n",
    "2. Before completing the incomplete `transform()` function, read through the entire code and comments to get an overview of how the three steps - Extract, Transform, and Load - are implemented. \n",
    "\n",
    "3. Complete the `transform()` function by copying part of the code that you completed in [Exercise 4](#ex04). \n",
    "\n",
    "4. Save changes to the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.2'></a>\n",
    "### 4.2 - Creating and Running the Glue Jobs\n",
    "\n",
    "After completing the scripts, you will now create the resources needed to run the Glue jobs using Terraform. Once you create the resources, you will run the AWS Glue Jobs to ingest the raw data from the source bucket and transform it. You are provided with the Terraform configuration files to create the Glue jobs, your task is to deploy the jobs. For a detailed overview of the provided terraform files, make sure to watch the lab walkthrough video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.1. Open the `./terraform/glue.tf` file. You will see two different Glue jobs: one for the reviews and another for the product metadata. For now, leave the file as is. Note the use of the `Snappy` compression algorithm for both jobs and the choice of partitioning columns: `year` and `month` for the reviews, and `sales_category` for the product metadata. In the optional part of the lab at the end, you will make changes to the compression algorithm and partitioning columns to perform various experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.2. In the Cloud9 or Jupyter terminal, navigate to the `terraform` folder, initialize Terraform and then deploy the resources by running the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "cd terraform\n",
    "terraform init\n",
    "terraform plan\n",
    "terraform apply\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: After the `terraform apply` command, you will need to type `yes` and press `Enter` to confirm that you want Terraform to apply the changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will include the name of each of the glue jobs and the IAM role used, as follows:\n",
    "\n",
    "```bash\n",
    "glue_role = \"Cloud9-de-c3w2lab1-glue-role\"\n",
    "metadata_glue_job = \"de-c3w2lab1-metadata-etl-job\"\n",
    "reviews_glue_job = \"de-c3w2lab1-reviews-etl-job\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These outputs will be used later in the lab, you can save them locally. Note that you will only need the values of the job names (i.e., `de-c3w2lab1-metadata-etl-job` and `de-c3w2lab1-reviews-etl-job`) for the next step. The key names (`metadata_glue_job` and `reviews_glue_job`) are provided for reference but are not used in the commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.3. Run each job with the command below, replacing the placeholder `<GLUE-JOB-NAME>` with the job name from the Terraform output (`de-c3w2lab1-reviews-etl-job` or `de-c3w2lab1-metadata-etl-job`):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "aws glue start-job-run --job-name <GLUE-JOB-NAME> | jq -r '.JobRunId' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "4.2.4. You can check the status of each Glue job in the console, or from the terminal by running the following command. Make sure to exchange the `<GLUE-JOB-NAME>` with the job name, and `<JOB-RUN-ID>` with the output from the previous step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "```bash\n",
    "aws glue get-job-run --job-name <GLUE-JOB-NAME> --run-id <JOB-RUN-ID> --output text --query \"JobRun.JobRunState\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "When each job has a `SUCCEEDED` status, you can continue with the rest of the lab. Take into account that each run of `de-c3w2lab1-metadata-etl-job` can take around 3 minutes while each run of `de-c3w2lab1-reviews-etl-job` can take between 7 to 8 minutes.\n",
    "- The processed metadata will be stored at: `s3://<BUCKET_NAME>/processed_data/snappy/partition_by_sales_category/toys_metadata/`.\n",
    "- The processed reviews will be stored at:\n",
    "`s3://<BUCKET_NAME>/processed_data/snappy/partition_by_year_month/toys_reviews/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Data Catalog with Glue Crawler\n",
    "\n",
    "AWS Glue Crawler is a powerful automated tool offered by AWS Glue for discovering and cataloging metadata about data sources, which enables services like Glue and Athena to query data directly from different sources. By simply pointing the crawler to your data source, whether it's a database or a data lake, it will automatically scan and extract the schema information, data types, and other relevant metadata. This metadata is then organized and stored in a database table in the AWS Glue Data Catalog, providing a centralized repository for understanding and managing your data assets. In this lab, you will create a crawler that will be in charge of populating the Glue Data Catalog with the newly transformed data in S3, you will be able to query the data directly from S3 using Athena in the final part.\n",
    "\n",
    "Start by checking the databases in the data catalog using AWS SDK for pandas (`awswrangler`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Database, Description]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "databases = wr.catalog.databases()\n",
    "print(databases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data catalog is empty, let's create a new catalog database labeled as `de-c3w2lab1-aws-reviews` in which the crawler will store the metadata of the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_NAME = \"de-c3w2lab1-aws-reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Database Description\n",
      "0  de-c3w2lab1-aws-reviews            \n"
     ]
    }
   ],
   "source": [
    "if DATABASE_NAME not in databases.values:\n",
    "    wr.catalog.create_database(DATABASE_NAME)\n",
    "    print(wr.catalog.databases())\n",
    "else:\n",
    "    print(f\"Database {DATABASE_NAME} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex07'></a>\n",
    "### Exercise 7\n",
    "\n",
    "Complete the code to create a Glue crawler utilizing the `boto3` library:\n",
    "\n",
    "1. Define the Role parameter with the Glue Job role that you obtained from the `terraform` output (step 4.2.2).\n",
    "2. Add a description to the crawler (e.g. 'Amazon Reviews for toys').\n",
    "3. Define the S3 targets with the  paths in your data lake bucket for the `toys_reviews` table (partitioned by year and month and with `snappy` compression) and the `toys_metadata` table (partitioned by sales category with `snappy` compression). You just need to replace `None` with the `BUCKET_NAME` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client('glue',region_name=\"us-east-1\")\n",
    "configuration= {\"Version\": 1.0,\"Grouping\": {\"TableGroupingPolicy\": \"CombineCompatibleSchemas\" }}\n",
    "\n",
    "response = glue_client.create_crawler(\n",
    "    Name='de-c3w2lab1-crawler',\n",
    "    ### START CODE HERE ### (12 lines of code)\n",
    "    Role= 'Cloud9-de-c3w2lab1-glue-role',\n",
    "    DatabaseName=DATABASE_NAME, \n",
    "    Description= 'Amazon Reviews for toys',\n",
    "    Targets={ \n",
    "        'S3Targets': [ \n",
    "            { \n",
    "                'Path': f's3://{BUCKET_NAME}/processed_data/snappy/partition_by_year_month/toys_reviews/',\n",
    "            },\n",
    "            { \n",
    "                'Path': f's3://{BUCKET_NAME}/processed_data/snappy/partition_by_sales_category/toys_metadata/',\n",
    "            } \n",
    "        ]} \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the crawler was indeed created, using the `list_crawlers` method of the `boto3` Glue Client. If the creation was successful, you should see the new crawler in the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de-c3w2lab1-crawler']\n"
     ]
    }
   ],
   "source": [
    "response = glue_client.list_crawlers()\n",
    "print(response['CrawlerNames'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the crawler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'f74ad6e8-e949-48d5-bc67-d293749b0a60', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 10 Dec 2024 11:54:09 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2', 'connection': 'keep-alive', 'x-amzn-requestid': 'f74ad6e8-e949-48d5-bc67-d293749b0a60', 'cache-control': 'no-cache'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = glue_client.start_crawler(\n",
    "    Name='de-c3w2lab1-crawler'\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crawler will start looking for data sources in the S3 targets that we have defined, it should take around **3 minutes** for the first run. \n",
    "\n",
    "After around 3 minutes, check that the two tables were created for each processed dataset using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>Table</th>\n",
       "      <th>Description</th>\n",
       "      <th>TableType</th>\n",
       "      <th>Columns</th>\n",
       "      <th>Partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>de-c3w2lab1-aws-reviews</td>\n",
       "      <td>toys_metadata</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>asin, description, title, price, brand, sales_...</td>\n",
       "      <td>sales_category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>de-c3w2lab1-aws-reviews</td>\n",
       "      <td>toys_reviews</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>reviewerid, asin, reviewername, reviewtext, ov...</td>\n",
       "      <td>year, month</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Database          Table Description       TableType  \\\n",
       "0  de-c3w2lab1-aws-reviews  toys_metadata              EXTERNAL_TABLE   \n",
       "1  de-c3w2lab1-aws-reviews   toys_reviews              EXTERNAL_TABLE   \n",
       "\n",
       "                                             Columns      Partitions  \n",
       "0  asin, description, title, price, brand, sales_...  sales_category  \n",
       "1  reviewerid, asin, reviewername, reviewtext, ov...     year, month  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.tables(database=DATABASE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to the last section of the lab where you will use Amazon Athena, make sure that two catalog tables were created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Data Querying with Athena\n",
    "\n",
    "Amazon Athena is a serverless, interactive query service provided by AWS, allowing you to analyze data in Amazon S3 using standard SQL. It enables you to quickly and easily query vast amounts of data stored in various formats such as CSV, JSON, Parquet, and more, without needing to set up and manage complex infrastructure. Athena leverages the AWS Glue Data Catalog to access the stored metadata associated with the S3 data. By utilizing this metadata, Athena seamlessly executes queries on the underlying data, streamlining the analytical process and enabling efficient data exploration and analysis.\n",
    "\n",
    "You will now test some of the queries that the data analyst will run. You will use the `awswrangler` library to run these queries.\n",
    "\n",
    "*Note*: `awswrangler` uses the `pyarrow` library that has some functions that returns a `FutureWarning` due to deprecation, this doesn't affect what you are trying to do so you will filter out those warnings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this test query to retrieve a sample of 5 records from the toys_reviews table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerid</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewername</th>\n",
       "      <th>reviewtext</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixreviewtime</th>\n",
       "      <th>reviewtime</th>\n",
       "      <th>helpful</th>\n",
       "      <th>totalhelpful</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3KZXJBA5XHEFB</td>\n",
       "      <td>B003H9MTJ6</td>\n",
       "      <td>frmrradiodjSHAY \"frmrradiodjSHAY\"</td>\n",
       "      <td>Our 10 year-old used his money, after saving f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sorry, Not All It's Cracked Up To Be</td>\n",
       "      <td>1224288000</td>\n",
       "      <td>2008-10-18</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>2008</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2Y4MHP9ZHTZID</td>\n",
       "      <td>B004BEG5VY</td>\n",
       "      <td>flarty blimptoads</td>\n",
       "      <td>WOW! What a great decoration! This was a huge ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>GREAT DECORATION!!!!</td>\n",
       "      <td>1225411200</td>\n",
       "      <td>2008-10-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A14O2EH46KXDMN</td>\n",
       "      <td>B003H4R2M0</td>\n",
       "      <td>CJ \"CJ\"</td>\n",
       "      <td>Everyone knows beanie babies, and I knew I had...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I love wonder pets!</td>\n",
       "      <td>1224028800</td>\n",
       "      <td>2008-10-15</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1H9XEXRW4OYIK</td>\n",
       "      <td>B003H9MTJ6</td>\n",
       "      <td>A. Hill</td>\n",
       "      <td>This gun is very cool! My son got it and he lo...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best Nerf gun yet!</td>\n",
       "      <td>1223769600</td>\n",
       "      <td>2008-10-12</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3PJOX3UHW3TZE</td>\n",
       "      <td>B003BLC6LQ</td>\n",
       "      <td>Paul Oncescu</td>\n",
       "      <td>I have purchased the toy for my 5 years old da...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great toy</td>\n",
       "      <td>1223251200</td>\n",
       "      <td>2008-10-06</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerid        asin                       reviewername  \\\n",
       "0  A3KZXJBA5XHEFB  B003H9MTJ6  frmrradiodjSHAY \"frmrradiodjSHAY\"   \n",
       "1  A2Y4MHP9ZHTZID  B004BEG5VY                  flarty blimptoads   \n",
       "2  A14O2EH46KXDMN  B003H4R2M0                            CJ \"CJ\"   \n",
       "3  A1H9XEXRW4OYIK  B003H9MTJ6                            A. Hill   \n",
       "4  A3PJOX3UHW3TZE  B003BLC6LQ                       Paul Oncescu   \n",
       "\n",
       "                                          reviewtext  overall  \\\n",
       "0  Our 10 year-old used his money, after saving f...      1.0   \n",
       "1  WOW! What a great decoration! This was a huge ...      5.0   \n",
       "2  Everyone knows beanie babies, and I knew I had...      5.0   \n",
       "3  This gun is very cool! My son got it and he lo...      5.0   \n",
       "4  I have purchased the toy for my 5 years old da...      5.0   \n",
       "\n",
       "                                summary  unixreviewtime reviewtime  helpful  \\\n",
       "0  Sorry, Not All It's Cracked Up To Be      1224288000 2008-10-18       11   \n",
       "1                  GREAT DECORATION!!!!      1225411200 2008-10-31        0   \n",
       "2                   I love wonder pets!      1224028800 2008-10-15        2   \n",
       "3                    Best Nerf gun yet!      1223769600 2008-10-12        5   \n",
       "4                             Great toy      1223251200 2008-10-06        5   \n",
       "\n",
       "   totalhelpful  year month  \n",
       "0            16  2008    10  \n",
       "1             0  2008    10  \n",
       "2             4  2008    10  \n",
       "3             7  2008    10  \n",
       "4             5  2008    10  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"SELECT * FROM toys_reviews LIMIT 5\"\n",
    "df = wr.athena.read_sql_query(sql, database=DATABASE_NAME, s3_output=f's3://{BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run this test query to find the top 5 products with the most reviews, return the name of the product and the count of reviews. Ignore the products with an empty title in the metadata table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cards Against Humanity</td>\n",
       "      <td>10281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Syma S107/S107G R/C Helicopter *Colors Vary</td>\n",
       "      <td>2938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Syma S107G 3.5 Channel RC Heli with Gyro - Yellow</td>\n",
       "      <td>2247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cards Against Humanity: First Expansion</td>\n",
       "      <td>2159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Syma S107/S107G R/C Helicopter | Red</td>\n",
       "      <td>2062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  review_count\n",
       "0                             Cards Against Humanity         10281\n",
       "1        Syma S107/S107G R/C Helicopter *Colors Vary          2938\n",
       "2  Syma S107G 3.5 Channel RC Heli with Gyro - Yellow          2247\n",
       "3            Cards Against Humanity: First Expansion          2159\n",
       "4               Syma S107/S107G R/C Helicopter | Red          2062"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT met.title, count(distinct toy.reviewerid) as review_count\n",
    "FROM toys_metadata met \n",
    "LEFT JOIN toys_reviews toy\n",
    "ON met.asin = toy.asin\n",
    "WHERE met.title <> ''\n",
    "GROUP BY met.title\n",
    "ORDER BY count(distinct toy.reviewerid) DESC\n",
    "LIMIT 5\"\"\"\n",
    "df = wr.athena.read_sql_query(sql, database=DATABASE_NAME, s3_output=f's3://{BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the next test query to find the top 10 products in terms of average rating, but the products should have at least 1000 reviews. Return the title, sales category and the average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>sales_category</th>\n",
       "      <th>review_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cards Against Humanity: Third Expansion</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>4.898524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cards Against Humanity</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>4.883669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Snap Circuits Jr. SC-100 Kit</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>4.849903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cards Against Humanity: Second Expansion</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>4.848140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cards Against Humanity: First Expansion</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>4.831403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title sales_category  review_avg\n",
       "0   Cards Against Humanity: Third Expansion   Toys & Games    4.898524\n",
       "1                    Cards Against Humanity   Toys & Games    4.883669\n",
       "2              Snap Circuits Jr. SC-100 Kit   Toys & Games    4.849903\n",
       "3  Cards Against Humanity: Second Expansion   Toys & Games    4.848140\n",
       "4   Cards Against Humanity: First Expansion   Toys & Games    4.831403"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT met.title, met.sales_category, avg(toy.overall) as review_avg\n",
    "FROM toys_metadata met \n",
    "LEFT JOIN toys_reviews toy\n",
    "ON met.asin = toy.asin\n",
    "GROUP BY met.title, met.sales_category\n",
    "HAVING count(distinct toy.reviewerid) > 1000\n",
    "ORDER BY avg(toy.overall) DESC\n",
    "LIMIT 10\"\"\"\n",
    "df = wr.athena.read_sql_query(sql, database=DATABASE_NAME, s3_output=f's3://{BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run this query to determine the average rating for each brand and the number of products they have in the database. Only show the top 10 brands with the highest product counts in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>product_count</th>\n",
       "      <th>review_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mattel</td>\n",
       "      <td>6171</td>\n",
       "      <td>4.180501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yu-Gi-Oh!</td>\n",
       "      <td>4825</td>\n",
       "      <td>4.407995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LEGO</td>\n",
       "      <td>3726</td>\n",
       "      <td>4.535898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Disney</td>\n",
       "      <td>3617</td>\n",
       "      <td>4.072743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hasbro</td>\n",
       "      <td>3533</td>\n",
       "      <td>4.084331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       brand  product_count  review_avg\n",
       "0     Mattel           6171    4.180501\n",
       "1  Yu-Gi-Oh!           4825    4.407995\n",
       "2       LEGO           3726    4.535898\n",
       "3     Disney           3617    4.072743\n",
       "4     Hasbro           3533    4.084331"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT met.brand, count(distinct met.asin) as product_count, avg(toy.overall) as review_avg\n",
    "FROM toys_metadata met \n",
    "LEFT JOIN toys_reviews toy\n",
    "ON met.asin = toy.asin\n",
    "WHERE met.brand <> ''\n",
    "GROUP BY met.brand\n",
    "ORDER BY count(distinct toy.asin) DESC\n",
    "LIMIT 10\"\"\"\n",
    "df = wr.athena.read_sql_query(sql, database=DATABASE_NAME, s3_output=f's3://{BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last test query, you want to look at 25 random reviews that gave a rating of 5 to a toy car product. Return the product title, description, the review text and overall score. Ignore products that have an empty title, use the LIKE operator to search reviews with toy car in their text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>reviewtext</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fisher-Price Little People Disney Princess: Ar...</td>\n",
       "      <td>You&amp;#x2019;re going to love watching your litt...</td>\n",
       "      <td>Granddaughter loves it!  Perfect addition to t...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.6 Volt Nitro NiCd, 700mAh Battery and Charger</td>\n",
       "      <td>NiCd.\n",
       "700mAh/9.6V.\n",
       "For use with any radio cont...</td>\n",
       "      <td>I plugged it in and charged it...and it worked...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12 Poly Non-Woven Tote Bags 6&amp;quot; Assorted C...</td>\n",
       "      <td>Bright Tote Bags. Fabulously chic and petite, ...</td>\n",
       "      <td>I thought these were a really good deal and th...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Plan Toys City Series Parking Garage</td>\n",
       "      <td>Plan Toys create to inspire children's imagina...</td>\n",
       "      <td>We almost bought a hot wheels garage instead o...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Swivel Car Rolling Ride On Toy - Indoor / Outd...</td>\n",
       "      <td>Safe and Fun for All AgesLet's be honest, some...</td>\n",
       "      <td>This toy car is fun however if you have hardwo...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Fisher-Price Little People Disney Princess: Ar...   \n",
       "1    9.6 Volt Nitro NiCd, 700mAh Battery and Charger   \n",
       "2  12 Poly Non-Woven Tote Bags 6&quot; Assorted C...   \n",
       "3               Plan Toys City Series Parking Garage   \n",
       "4  Swivel Car Rolling Ride On Toy - Indoor / Outd...   \n",
       "\n",
       "                                         description  \\\n",
       "0  You&#x2019;re going to love watching your litt...   \n",
       "1  NiCd.\n",
       "700mAh/9.6V.\n",
       "For use with any radio cont...   \n",
       "2  Bright Tote Bags. Fabulously chic and petite, ...   \n",
       "3  Plan Toys create to inspire children's imagina...   \n",
       "4  Safe and Fun for All AgesLet's be honest, some...   \n",
       "\n",
       "                                          reviewtext  overall  \n",
       "0  Granddaughter loves it!  Perfect addition to t...      5.0  \n",
       "1  I plugged it in and charged it...and it worked...      5.0  \n",
       "2  I thought these were a really good deal and th...      5.0  \n",
       "3  We almost bought a hot wheels garage instead o...      5.0  \n",
       "4  This toy car is fun however if you have hardwo...      5.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT met.title, met.description, toy.reviewtext, toy.overall\n",
    "FROM toys_reviews toy\n",
    "LEFT JOIN toys_metadata met\n",
    "ON toy.asin = met.asin\n",
    "WHERE toy.reviewtext like '%toy car%' and toy.overall = 5.0 and met.title <> '' \n",
    "LIMIT 25\"\"\"\n",
    "df = wr.athena.read_sql_query(sql, database=DATABASE_NAME, s3_output=f's3://{BUCKET_NAME}/athena_output/')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "The next section is optional. If you'd like to try it, first ensure you submit the lab for grading, and then proceed with the experiments. Otherwise, feel free to skip to [the last section](#8). For a summary of the optional section, you can watch the lab walkthrough video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Optional Experiments: Partitioning and Compression Features of Parquet Format\n",
    "\n",
    "**Compression**\n",
    "\n",
    "The Parquet format, commonly used in data lake architectures, supports various [compression codecs](https://parquet.apache.org/docs/file-format/data-pages/compression/) such as Snappy, Gzip, and LZO. These codecs play a crucial role in reducing the storage space needed by Parquet files. However, they come with a trade-off: while they reduce storage costs and optimize resource utilization, they can also impact processing speeds during data ingestion, transformation, and querying. To test this with a practical example, you will store the same transformed data using three different compression options: `UNCOMPRESSED`, `SNAPPY` and `GZIP`: \n",
    "\n",
    "- `UNCOMPRESSED`: Data is left uncompressed.\n",
    "- `SNAPPY`: Snappy is a fast compression/decompression library that works well with Parquet files. It provides good compression ratios and is optimized for speed. Snappy compression is often a good choice for balancing compression efficiency and query performance.\n",
    "- `GZIP`: Gzip is a widely used compression algorithm that provides high compression ratios. However, it tends to be slower in both compression and decompression compared to Snappy. Gzip compression can achieve higher levels of compression but may result in slower query performance.\n",
    "\n",
    "**Partitioning**\n",
    "\n",
    "Partitioning in Parquet is a technique used to better organize data within Parquet files based on partition keys. By partitioning data, Parquet optimizes query performance by reducing the amount of data that needs to be scanned during query execution. \n",
    "\n",
    "**Experiments**\n",
    "\n",
    "To explore the compression features, you will process the product metadata 3 times; in each time, you will choose a different option for the compression algorithm with no partitioning as shown in the left table here. Then you will compare for this data, \"uncompressed versus Snappy\" and then \"Snappy versus Gzip\". \n",
    "\n",
    "To explore the partitioning features, you will process the review dataset 3 times; in each time, you will choose a different option for the partitioning column, with Snappy for compression. Then you will compare for this data, \"partitioning versus no partitioning\" and then \"partitioning by year and month\" versus \"partitioning by the product id (asin)\".\n",
    "\n",
    "<img src=\"images/experiment.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7.1'></a>\n",
    "### 7.1 - Experiments with the Glue Jobs\n",
    "\n",
    "Open the file `./terraform/glue.tf` where you will be specifying the parameters for each of the experiments. Remember to save changes by pressing `Ctrl+S` or `Cmd+S` before redeploying each job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='exp01'></a>\n",
    "### Metadata - No Partitioning, Uncompressed\n",
    "\n",
    "Search for the `resource \"aws_glue_job\" \"metadata_etl_job\"` terraform resource. In the `default_arguments` parameter, set the following:\n",
    "- `--compression` to `\"uncompressed\"` indicating that the parquet files will be saved without any compression,\n",
    "- `--partition_cols` to `jsonencode([])`, which will be an empty list in the partition columns, no partitioning will be performed. \n",
    "\n",
    "Save the changes and follow steps 4.2.2 - 4.2.4. Run only the metadata Glue job. The results of this experiment will be stored at `s3://<BUCKET_NAME>/processed_data/uncompressed/no_partition/toys_metadata/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='exp02'></a>\n",
    "### Metadata - No Partitioning, Snappy\n",
    "\n",
    "Search for the `resource \"aws_glue_job\" \"metadata_etl_job\"` terraform resource. In the `default_arguments` parameter, set \n",
    "- `--compression` to `\"snappy\"` indicating that the parquet files will be compressed using the `SNAPPY` algorithm,\n",
    "- `--partition_cols` to `jsonencode([])`. \n",
    "\n",
    "Again, save the changes and follow steps 4.2.2 - 4.2.4. Run only the metadata Glue job. The results of this experiment will be stored at `s3://<BUCKET_NAME>/processed_data/snappy/no_partition/toys_metadata/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='exp03'></a>\n",
    "### Metadata - No partitioning, Gzip\n",
    "\n",
    "Search for the `resource \"aws_glue_job\" \"metadata_etl_job\"` terraform resource. In the `default_arguments` parameter, set \n",
    "- `--compression` to `\"gzip\"` indicating that the parquet files will be compressed using the `GZIP` algorithm,\n",
    "- `--partition_cols` to `jsonencode([])`. \n",
    "\n",
    "Again, save the changes and follow steps 4.2.2 - 4.2.4. Run only the metadata Glue job. The results of this experiment will be stored at `s3://<BUCKET_NAME>/processed_data/gzip/no_partition/toys_metadata/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='exp04'></a>\n",
    "### Reviews - No partitioning, Snappy\n",
    "\n",
    "Search for the `resource \"aws_glue_job\" \"reviews_etl_job\"` terraform resource. In the `default_arguments` parameter, set \n",
    "- `--compression` to `\"snappy\"`; this will indicate the job that the parquet files will be compressed using the `SNAPPY` algorithm,\n",
    "- `--partition_cols` to `jsonencode([])`.\n",
    "\n",
    "Run only the review Glue job. After running the Glue Job, the results of this experiment will be stored at `s3://<BUCKET_NAME>/processed_data/snappy/no_partition/toys_reviews/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='exp05'></a>\n",
    "### Reviews - Partitioning by year and month, Snappy\n",
    "\n",
    "Here you don't need to run any glue jobs. You've already run this Glue job with Snappy as the compression algorithm and using the year and month columns as the partitioning key. The results are stored at `s3://<BUCKET_NAME>/processed_data/snappy/partition_by_year_month/toys_reviews/`.\n",
    "The parquet files are partitioned by the value of the two columns, year and month. The first partition is done by `year`and then, each `year` is partitioned into sub-partitions according to the `month`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='exp06'></a>\n",
    "### Reviews - Partitioning by asin, Snappy\n",
    "\n",
    "Search for the `resource \"aws_glue_job\" \"reviews_etl_job\"` terraform resource. In the `default_arguments` parameter, set \n",
    "- `--compression` to `\"snappy\"`; \n",
    "- `--partition_cols` to `jsonencode([\"asin\"])`. \n",
    "\n",
    "Create and run only the review Glue Job. Take into account that this experiment is expected to fail due to a **timeout** (It will run for 15 minutes and then stop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7.2'></a>\n",
    "### 7.2 - Experiment Results\n",
    "\n",
    "Now that your jobs have succeeded, and the processed data of each experiment has been stored in its corresponding locations, in this section you'll analyze the results.\n",
    "\n",
    "**Compressed vs Uncompressed data**\n",
    "\n",
    "First, let's compare between uncompressed and compressed data with no partitions using Metadata. Run the following commands to take a look to the size of the stored files. Remember that you can also run these commands in the Cloud9 terminal.\n",
    "\n",
    "Read the file sizes of Metadata - No partitioning, Uncompressed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --summarize --human-readable --recursive s3://{BUCKET_NAME}/processed_data/uncompressed/no_partition/toys_metadata/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the file sizes of Metadata - No partitioning, Snappy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --summarize --human-readable --recursive s3://{BUCKET_NAME}/processed_data/snappy/no_partition/toys_metadata/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that uncompressed files are bigger in size than the compressed ones, by almost a factor of 2. Also, even though you didn't specify a partition key, there are several files instead of only one. Some processing frameworks such as Apache Spark and AWS Glue will by default try to partition your data automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Snappy vs Gzip**\n",
    "\n",
    "Now, let's take a look at the results from applying two different compression algorithms.Run the following commands.\n",
    "\n",
    "Read the file sizes of Metadata - No partitioning, Snappy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --summarize --human-readable --recursive s3://{BUCKET_NAME}/processed_data/snappy/no_partition/toys_metadata/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the file sizes of Metadata - No partitioning, Gzip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --summarize --human-readable --recursive s3://{BUCKET_NAME}/processed_data/gzip/no_partition/toys_metadata/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the compression algorithms, you can see how `GZIP` has a better compression rate than `SNAPPY`. However, this higher compression rate comes at the cost of being less efficient at querying the data when using `GZIP`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partition keys**\n",
    "\n",
    "Finally, let's compare the usage of partition keys with the reviews dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No partition keys vs using partition keys**- Let's look again at the results of Reviews - Partitioning by year and month versus no_partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --summarize --human-readable --recursive s3://{BUCKET_NAME}/processed_data/snappy/no_partition/toys_reviews/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --summarize --human-readable --recursive s3://{BUCKET_NAME}/processed_data/snappy/partition_by_year_month/toys_reviews/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the partitioned results, you will find a bunch of folders with a structure similar this:\n",
    "\n",
    "```bash\n",
    ".  \n",
    " processed_data/\n",
    "     snappy/\n",
    "         partition_by_year_month/\n",
    "             toys_reviews/\n",
    "                 year = <value-1>/\n",
    "                |    month = <value-x>\n",
    "                |   |     run-<TIMESTAMP>-part-block-0-0-r-00000-snappy.parquet\n",
    "                |   |     run-<TIMESTAMP>-part-block-0-0-r-00001-snappy.parquet\n",
    "                |   |    ...\n",
    "                |   |     run-<TIMESTAMP>-part-block-0-0-r-<PARTITION-ID>-snappy.parquet\n",
    "                |   ...\n",
    "                |    month = <value-z>\n",
    "                |         run-<TIMESTAMP>-part-block-0-0-r-00000-snappy.parquet\n",
    "                |         run-<TIMESTAMP>-part-block-0-0-r-00001-snappy.parquet\n",
    "                |        ...\n",
    "                |         run-<TIMESTAMP>-part-block-0-0-r-<PARTITION-ID>-snappy.parquet\n",
    "                ...                \n",
    "                 year = <value-n>/\n",
    "                     month = <value-x>\n",
    "                    |     run-<TIMESTAMP>-part-block-0-0-r-00000-snappy.parquet\n",
    "                    |     run-<TIMESTAMP>-part-block-0-0-r-00001-snappy.parquet\n",
    "                    |    ...\n",
    "                    |     run-<TIMESTAMP>-part-block-0-0-r-<PARTITION-ID>-snappy.parquet\n",
    "                    ...\n",
    "                     month = <value-z>\n",
    "                          run-<TIMESTAMP>-part-block-0-0-r-00000-snappy.parquet\n",
    "                          run-<TIMESTAMP>-part-block-0-0-r-00001-snappy.parquet\n",
    "                         ...\n",
    "                          run-<TIMESTAMP>-part-block-0-0-r-<PARTITION-ID>-snappy.parquet\n",
    "```\n",
    "\n",
    "The folders you see have names made up of the partition key (or column name) and the corresponding partition value. Inside each folder, you'll find one or more parquet files with particular IDs. You can see that the total size is quite similar. \n",
    "\n",
    "Partition keys must be set in a way that distributes the data as evenly as possible across the partitions, as you will see in the next experiment.\n",
    "\n",
    "**Setting appropriate partition keys**:\n",
    "Now, let's take a look at the results of the last Glue job. The job will run for 15 minutes, and then you will get a timeout error if you check the status of the glue job in the console.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --summarize --human-readable --recursive s3://{BUCKET_NAME}/processed_data/snappy/partition_by_year_month/toys_reviews/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The execution of this cell may take around 1-2 minutes:\n",
    "!aws s3 ls --summarize --human-readable --recursive s3://{BUCKET_NAME}/processed_data/snappy/partition_by_asin/toys_reviews/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the final output of these last two commands, you can see that using the year and month columns as partition keys generated around 556 files with a total size of less than 600MB. On the other hand, using the asin column (which is the identifier for each product) as the partition key can generate thousands of files. However, this job failed due to a timeout, resulting in a smaller total size as not all data was saved. This illustrates one of the issues that may arise when using inappropriate partition keys.\n",
    "\n",
    "With year and month as partition keys, the number of reviews can be more evenly distributed across dates, resulting in a manageable number of files with sizes which can be later easily processed. However, with the asin partition key, given that some products may have very few reviews and there's a large number of products, the Glue job couldn't save all files within the allotted time.\n",
    "\n",
    "Having too many small files can have several disadvantages:\n",
    "\n",
    "* Increased Metadata Overhead: Each Parquet file carries metadata. With numerous small files, managing this metadata can significantly impact performance.\n",
    "* Higher Storage Costs: Storing many small files can result in higher costs compared to fewer larger files, as cloud storage providers often charge based on the number of objects stored.\n",
    "* Slower Filesystem Operations: Operations like listing or opening files may become slower with numerous small files, affecting performance of administrative tasks or data processing.\n",
    "* Suboptimal Data Processing: Some data processing frameworks may be less efficient when handling many small files, incurring overhead in file opening, network communication, and task scheduling.\n",
    "\n",
    "The key is to balance partitioning data for query performance while avoiding the drawbacks of too many small files. It's recommended to choose a partitioning strategy that aligns with your querying patterns and dataset scale, considering factors like storage costs, filesystem limitations, and processing efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='8'></a>\n",
    "## 8 - Clean up\n",
    "\n",
    "After finishing the experiments and exercises, you will have to delete some of the created resources, let's start with the resources created with terraform. Run the following command inside the `terraform` folder:\n",
    "\n",
    "```bash\n",
    "terraform destroy\n",
    "```\n",
    "\n",
    "Now, let's delete the AWS Glue crawler and database, run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = glue_client.delete_crawler(\n",
    "        Name='de-c3w2lab1-crawler'\n",
    "    )\n",
    "    print(\"Crawler deleted successfully\")\n",
    "except glue_client.exceptions.EntityNotFoundException:\n",
    "    print(\"Crawler does not exist or has already been deleted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wr.catalog.delete_database(name=DATABASE_NAME)\n",
    "    print(f\"Database {DATABASE_NAME} deleted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting database: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, verify that the resources have been deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue_client.list_crawlers()\n",
    "print(\"Remaining crawlers:\", response['CrawlerNames'])\n",
    "\n",
    "databases = wr.catalog.databases()\n",
    "print(\"Remaining databases:\", databases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Well done!** In this lab, you implemented a simple data lake using Amazon S3, AWS Glue ETL, Glue Crawler, and Amazon Athena. You ingested raw data into an S3 bucket, which acted as the storage layer of the data lake. You then used AWS GLUE ETL to transform and prepare the data for analysis. After that, you used the Glue Crawler to catalog the data, and then used Athena to run SQL queries directly against the data lake without the need for complex data movement or transformation. This setup allows for easy and efficient querying of data stored in S3, enabling organizations to derive valuable insights from their data assets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
